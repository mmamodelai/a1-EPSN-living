name: 2. Process Living Data

on:
  schedule:
    - cron: '30 6 * * 1'  # Monday 6:30 AM UTC (after scraping)
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Run in test mode with 24 fighters (faster)'
        required: false
        default: true
        type: boolean

jobs:
  process-living-data:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Verify HTML files exist
      run: |
        echo "🔍 Checking HTML files..."
        
        if [ -d "data/FighterHTMLs" ]; then
          html_count=$(find data/FighterHTMLs -name "*.html" | wc -l)
          echo "✅ Found $html_count HTML files"
          
          if [ $html_count -eq 0 ]; then
            echo "❌ No HTML files found - run '1. Scrape ESPN' workflow first"
            exit 1
          fi
          
          # Show sample files
          echo "📋 Sample HTML files:"
          find data/FighterHTMLs -name "*.html" | head -5
        else
          echo "❌ FighterHTMLs directory not found - run '1. Scrape ESPN' workflow first"
          exit 1
        fi
        
    - name: Process HTML to living documents
      run: |
        echo "🔄 Processing HTML files to living documents..."
        echo "📋 This will create:"
        echo "   ✅ Striking Data (distance striking stats)"
        echo "   ✅ Clinch Data (clinch striking, grappling)"
        echo "   ✅ Ground Data (ground fighting stats)"
        echo ""
        
        # Create a Python script to process HTML files into living documents
        cat > process_html_to_living.py << 'EOF'
        #!/usr/bin/env python3
        """
        Process HTML files to create living documents
        """
        import pandas as pd
        import json
        import re
        from pathlib import Path
        from bs4 import BeautifulSoup
        import logging
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        def extract_fight_data_from_html(html_file):
            """Extract fight-by-fight data from ESPN HTML file"""
            try:
                with open(html_file, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                
                # Skip small files
                if len(html_content) < 10000:
                    return []
                
                # Extract fighter name from filename
                fighter_name = html_file.stem.replace('_', ' ')
                
                # Look for ESPN JSON data
                match = re.search(r'"prtlCmnApiRsp"\s*:\s*({.*?})\s*,\s*"pageType"', html_content, re.DOTALL)
                if not match:
                    return []
                
                json_str = match.group(1)
                data = json.loads(json_str)
                
                fights = []
                
                # Extract fight data from ESPN structure
                if 'ath' in data and 'stats' in data['ath']:
                    stats = data['ath']['stats']
                    
                    # Process each fight
                    for stat in stats:
                        if 'evt' in stat and 'opp' in stat:
                            fight_data = {
                                'Player': fighter_name,
                                'Date': stat.get('evt', {}).get('dt', ''),
                                'Opponent': stat.get('opp', {}).get('ath', {}).get('nm', ''),
                                'Event': stat.get('evt', {}).get('nm', 'ESPN Scraped'),
                                'Result': stat.get('rslt', ''),
                                'SGBL': stat.get('sgb', {}).get('lnd', 0),
                                'SGBA': stat.get('sgb', {}).get('att', 0),
                                'SGHL': stat.get('sgh', {}).get('lnd', 0),
                                'SGHA': stat.get('sgh', {}).get('att', 0),
                                'SGLL': stat.get('sgl', {}).get('lnd', 0),
                                'SGLA': stat.get('sgl', {}).get('att', 0),
                                'AD': stat.get('td', {}).get('lnd', 0),
                                'ADHG': stat.get('td', {}).get('hg', 0),
                                'ADTB': stat.get('td', {}).get('tb', 0),
                                'ADTM': stat.get('td', {}).get('tm', 0),
                                'ADTS': stat.get('td', {}).get('ts', 0),
                                'SM': stat.get('sm', 0),
                                'SA': stat.get('sa', 0),
                                'SL': stat.get('sl', 0)
                            }
                            fights.append(fight_data)
                
                return fights
                
            except Exception as e:
                logger.error(f"Error processing {html_file}: {e}")
                return []
        
        def main():
            """Main processing function"""
            html_folder = Path("data/FighterHTMLs")
            
            if not html_folder.exists():
                logger.error("HTML folder not found")
                return
            
            html_files = list(html_folder.glob("*.html"))
            logger.info(f"Processing {len(html_files)} HTML files")
            
            all_fights = []
            
            for html_file in html_files:
                fights = extract_fight_data_from_html(html_file)
                all_fights.extend(fights)
                logger.info(f"Extracted {len(fights)} fights from {html_file.name}")
            
            if all_fights:
                # Create DataFrame
                df = pd.DataFrame(all_fights)
                
                # Save to living documents
                df.to_csv("data/striking_data_living.csv", index=False)
                df.to_csv("data/clinch_data_living.csv", index=False)
                df.to_csv("data/ground_data_living.csv", index=False)
                
                logger.info(f"Created living documents with {len(df)} fight records")
            else:
                logger.warning("No fight data extracted")
        
        if __name__ == "__main__":
            main()
        EOF
        
        # Run the processing script
        python process_html_to_living.py
        
        echo "✅ Living data processing completed at $(date)"
        
    - name: Verify living documents
      run: |
        echo "🔍 Verifying living documents..."
        echo ""
        
        # Check CSV files
        echo "📊 Living Documents:"
        for file in data/striking_data_living.csv data/clinch_data_living.csv data/ground_data_living.csv; do
          if [ -f "$file" ]; then
            size=$(ls -lh "$file" | awk '{print $5}')
            lines=$(wc -l < "$file")
            echo "   $(basename "$file"): $lines records ($size)"
          else
            echo "   ❌ $(basename "$file"): Not found"
          fi
        done
        
        # Show recent data
        echo ""
        echo "📋 Recent fight data:"
        if [ -f "data/striking_data_living.csv" ]; then
          echo "Latest 3 fights from striking data:"
          tail -4 data/striking_data_living.csv | head -3
        fi
        
    - name: Create backup of living files
      run: |
        echo "💾 Creating backup of living files..."
        timestamp=$(date +"%Y%m%d_%H%M%S")
        mkdir -p "data/backups/backup_${timestamp}"
        
        for file in data/striking_data_living.csv data/clinch_data_living.csv data/ground_data_living.csv; do
          if [ -f "$file" ]; then
            cp "$file" "data/backups/backup_${timestamp}/"
            echo "✅ Backed up $(basename "$file")"
          fi
        done
        
        echo "✅ Backup created: backup_${timestamp}"
        
    - name: Commit and push living documents
      run: |
        echo "🚀 Committing living documents..."
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add living documents and backups
        git add data/striking_data_living.csv data/clinch_data_living.csv data/ground_data_living.csv || echo "No living documents to add"
        git add data/backups/ || echo "No backups to add"
        
        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "No changes to living documents to commit"
        else
          # Commit with timestamp
          timestamp=$(date +"%Y-%m-%d %H:%M:%S")
          git commit -m "📊 Living Data: Striking/Clinch/Ground - $timestamp"
          
          # Push changes
          git push
          echo "✅ Living documents committed and pushed"
        fi
        
    - name: Summary
      run: |
        echo ""
        echo "🎯 Living Data Processing Summary:"
        echo "📁 Repository: A1-ESPN-Profiles"
        echo "📋 Output: Striking, Clinch, Ground living documents"
        echo "⏰ Completed: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
        echo "🔄 Next step: Run '3. Process Fighter Profiles' workflow"
        echo "✅ Pipeline: Step 2 Complete" 